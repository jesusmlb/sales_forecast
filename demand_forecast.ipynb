{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusmlb/sales_forecast/blob/main/demand_forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5pxWhD12Ucy"
      },
      "source": [
        "# Predict the sales units of individual products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psqHdnpO2Uc0"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Modelling and Forecasting\n",
        "import sklearn\n",
        "import skforecast\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skforecast.recursive import ForecasterRecursiveMultiSeries\n",
        "from skforecast.recursive import ForecasterRecursive\n",
        "from skforecast.model_selection import TimeSeriesFold, OneStepAheadFold\n",
        "from skforecast.model_selection import backtesting_forecaster\n",
        "from skforecast.model_selection import bayesian_search_forecaster\n",
        "from skforecast.model_selection import backtesting_forecaster_multiseries\n",
        "from skforecast.model_selection import bayesian_search_forecaster_multiseries\n",
        "from skforecast.feature_selection import select_features_multiseries\n",
        "from skforecast.preprocessing import RollingFeatures\n",
        "from skforecast.exceptions import OneStepAheadValidationWarning\n",
        "from skforecast.plot import set_dark_theme\n",
        "from skforecast.preprocessing import series_long_to_dict\n",
        "from skforecast.preprocessing import exog_long_to_dict\n",
        "\n",
        "# Warnings configuration\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAdCbrI12Uc2"
      },
      "source": [
        "#### Let's import our dataset and have a look at the shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQmortgG2Uc2"
      },
      "outputs": [],
      "source": [
        "# Let's import our dataset\n",
        "df = pd.read_csv('MasterDataTEST_OP.csv', low_memory=False)\n",
        "\n",
        "# Let's check the shape of our dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hIPGZQE2Uc2"
      },
      "source": [
        "#### Let's check our dataset info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "texKzoKG2Uc3"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oULAitbp2Uc3"
      },
      "source": [
        "#### Before changing the datatype of our date columns, let's examine why there are so many null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAkeGYTm2Uc3"
      },
      "outputs": [],
      "source": [
        "# Let's check the whole row with non-null values of next_delivery_date_1\n",
        "df[df['next_delivery_date_1'].isna()].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieMaBSOA2Uc4"
      },
      "source": [
        "#### It is interesting that we don't have delivery dates for next_delivery_date_1, yet we have quantities to be delivered. How many such cases do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5VQNR6j2Uc4"
      },
      "outputs": [],
      "source": [
        "df[(df['next_delivery_quantity_1'] > 0) & (df['next_delivery_date_1'].isna())].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgbEKvfY2Uc4"
      },
      "outputs": [],
      "source": [
        "# Let's see the rows and evaluate the sku with nan values in dates\n",
        "df[(df['next_delivery_quantity_1'] > 0) & (df['next_delivery_date_1'].isna())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRdtk_fB2Uc4"
      },
      "outputs": [],
      "source": [
        "# Let's check how many weeks of weeks_to_next_delivery_1 for the product \"UWYXC\" usually has\n",
        "df[df['sku_id'] == 'UWYXC'].iloc[:, :20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ad0rJLD2Uc4"
      },
      "source": [
        "After checking the items in delivery_date_1 with null values, we recognized that delivery dates typically start when the order is placed. Therefore, we can fill these null values with the number of days the order has been waiting to arrive.\n",
        "\n",
        "For the other NaN values, we understand that they occur because there are no quantities to be delivered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-S4sy1p2Uc5"
      },
      "outputs": [],
      "source": [
        "# Function to update 'weeks_to_next_delivery_1' for filtered rows in each SKU\n",
        "def fill_weeks_to_next_delivery(group, col):\n",
        "    # Define the filter condition within the group\n",
        "    group_condition = (group[f'next_delivery_quantity_{col}'] > 0) & (group[f'next_delivery_date_{col}'].isna())\n",
        "    filtered_group = group[group_condition]\n",
        "    if not filtered_group.empty:\n",
        "        # Determine the number of rows matching the condition\n",
        "        num_rows = len(filtered_group)\n",
        "        # Create a countdown sequence from num_rows to 1 for filtered rows\n",
        "        group.loc[filtered_group.index, f'weeks_to_next_delivery_{col}'] = range(num_rows, 0, -1)\n",
        "    return group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uDEQmCe2Uc5"
      },
      "outputs": [],
      "source": [
        "# Apply the function to each SKU group\n",
        "df = df.groupby('sku_id', group_keys=False).apply(fill_weeks_to_next_delivery, col=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwtaXiai2Uc5"
      },
      "outputs": [],
      "source": [
        "# Let's see some rows to see the results of the functions\n",
        "df[(df['next_delivery_quantity_1'] > 0) & (df['next_delivery_date_1'].isna())].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhKzEtfE2Uc5"
      },
      "source": [
        "##### Great now let's add the dates to next_delivery_date_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFlgXUbU2Uc5"
      },
      "outputs": [],
      "source": [
        "# Because our cal_date is not in datetime format, we need to convert it first\n",
        "df['cal_date'] = pd.to_datetime(df['cal_date'])\n",
        "\n",
        "# Fill the next_delivery_date_1 with cal_date + weeks_to_next_delivery_1 but just dates without timestamp\n",
        "df['next_delivery_date_1'] = df['next_delivery_date_1'].fillna(df['cal_date'] + pd.to_timedelta(df['weeks_to_next_delivery_1'], unit='W'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJpWeP0p2Uc5"
      },
      "outputs": [],
      "source": [
        "# Let's have a look again to the null values\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRsWIspu2Uc5"
      },
      "outputs": [],
      "source": [
        "# Now let's do the same for the rest of the dates_dalivery\n",
        "for i in range(2, 6):\n",
        "    df = df.groupby('sku_id', group_keys=False).apply(fill_weeks_to_next_delivery, col=i)\n",
        "    df[f'next_delivery_date_{i}'] = df[f'next_delivery_date_{i}'].fillna(df['cal_date'] + pd.to_timedelta(df[f'weeks_to_next_delivery_{i}'], unit='W'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCr-inPd2Uc5"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulksvl82Uc5"
      },
      "source": [
        "#### Great, now that we have the data complete. Let's change the data type of the dates columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ9qH0bu2Uc5"
      },
      "outputs": [],
      "source": [
        "# Convert the next_delivery_date columns to datetime\n",
        "date_columns = [f'next_delivery_date_{i}' for i in range(1, 6)]\n",
        "df[date_columns] = df[date_columns].apply(pd.to_datetime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbZcjUBr2Uc6"
      },
      "outputs": [],
      "source": [
        "# Let's check again the datatypes of our columns\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfVLAnmA2Uc6"
      },
      "source": [
        "#### Let's check duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wikYkNMz2Uc6"
      },
      "outputs": [],
      "source": [
        "# Let's sum the duplicates in the dataset\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vBoiDWB2Uc6"
      },
      "outputs": [],
      "source": [
        "# Let's see the range of our cal_date\n",
        "df['cal_date'].min(), df['cal_date'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9tFuBVO2Uc6"
      },
      "outputs": [],
      "source": [
        "# And how many sku do we have in the dataset\n",
        "df['sku_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7deNDGz2Uc6"
      },
      "outputs": [],
      "source": [
        "# Let's create a basic summary of the dataset\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMw0CTUz2Uc6"
      },
      "source": [
        "#### The first thing we noticed was the presence of negative values in gsales_v. This could be due to several reasons, such as product returns, inventory adjustments, or data entry errors. We will examine the quantity of negative values in gsales_v and compare them with the inventory data to validate the first hypothesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikL8GTd_2Uc6"
      },
      "outputs": [],
      "source": [
        "# Why do we have negative gsales_v? Let's have a look\n",
        "df[df['gsales_v'] < 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5aRzRWp2Uc6"
      },
      "outputs": [],
      "source": [
        "# First we can see that the quantities (gsales_u) is 0. Let's pick one item to see the details\n",
        "\n",
        "df[df['sku_id'] == 'QCLLY'].iloc[40:80, [0, 1, 2, 3, 4]] # Let's pick just the columns we are interested in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgqIJdre2Uc6"
      },
      "source": [
        "#### We observe that after a negative sales value, the inventory shows an increase in the quantity of the product. This indicates a product return. We can keep these rows. Next, let's examine other products with negative sales values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmQHnBrD2Uc6"
      },
      "outputs": [],
      "source": [
        "# Group the data by SKU and then process each group to find comparisons\n",
        "grouped_comparisons = []\n",
        "\n",
        "for sku, group in df.groupby('sku_id'):\n",
        "    # Identify rows with negative `gsales_v` within the group\n",
        "    negative_gsales_v_group = group[group['gsales_v'] < 0]\n",
        "    # Get the previous rows for the negative `gsales_v` rows\n",
        "    previous_rows_group = group.shift(1).loc[negative_gsales_v_group.index]\n",
        "    # Combine for the group and append the result\n",
        "    comparison_group = pd.concat([previous_rows_group, negative_gsales_v_group], keys=['Previous', 'Current'])\n",
        "    comparison_group['sku_id'] = sku  # Ensure SKU is included\n",
        "    grouped_comparisons.append(comparison_group)\n",
        "\n",
        "# Combine all grouped comparisons into a single DataFrame\n",
        "grouped_comparisons_df = pd.concat(grouped_comparisons)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdCzDzIO2Uc7"
      },
      "outputs": [],
      "source": [
        "# Let's have a look at the grouped_comparisons_df\n",
        "grouped_comparisons_df.iloc[40:80, [0, 1, 2, 3, 4, 26, 27, 28, 29, 30, 33, 34, 35]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ZAt-Lf2Uc7"
      },
      "source": [
        "##### After examining other products, we recognized that not all of them follow the same logic of returns; some lack sales quantities or promotions applied. We can drop these rows from the analysis. It would be worthwhile to discuss with the business team to understand the reasons behind these negative values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpZpsYzg2Uc7"
      },
      "outputs": [],
      "source": [
        "# Filter rows where `gsales_v` is negative\n",
        "negative_gsales_v = df[df['gsales_v'] < 0]\n",
        "\n",
        "# Get the previous rows for the negative `gsales_v` DataFrame\n",
        "previous_rows = negative_gsales_v.groupby('sku_id').shift(1)\n",
        "\n",
        "# Compare `dc_reg_inventory_u` between current and previous rows for negative `gsales_v` rows\n",
        "rows_without_change = negative_gsales_v[\n",
        "    (negative_gsales_v['dc_reg_inventory_u'] == previous_rows['dc_reg_inventory_u'])\n",
        "]\n",
        "\n",
        "# Get the indices of rows with changes\n",
        "indices_without_change = rows_without_change.index\n",
        "\n",
        "# Let's drop from the dataset the rows with\n",
        "print(df.shape)\n",
        "df = df.drop(indices_without_change)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29g1O-9e2Uc7"
      },
      "source": [
        "#### Great now that we are ok with the negative value. Let's check the dc_reg_inventory_u and check why we are having negative values in the inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfhtLkqb2Uc7"
      },
      "outputs": [],
      "source": [
        "df[df['dc_reg_inventory_u'] < 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLjN9nd-2UdA"
      },
      "source": [
        "#### We observed that gsales_u has positive values, which could indicate that the product was sold but the system did not update the inventory (backorders). We will replace these negative values with zero, as we aim to preserve the product demand for forecasting purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt2WDc6q2UdA"
      },
      "outputs": [],
      "source": [
        "# Replace negative values in 'dc_reg_inventory_u' with 0\n",
        "df['dc_reg_inventory_u'] = df['dc_reg_inventory_u'].clip(lower=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM6QD5LK2UdA"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIlhrVFb2UdB"
      },
      "source": [
        "#### Our next column will be weeks_to_next_delivery_1 and _2 to. Let's see what happened with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvXs9q972UdB"
      },
      "outputs": [],
      "source": [
        "# Let's filter those sku_id with weeks_to_next_delivery_1 that are negative\n",
        "df[df['weeks_to_next_delivery_1'] < 0][\"sku_id\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8AM_Mh52UdB"
      },
      "outputs": [],
      "source": [
        "# Let's check this sku_id = OPCIZ\n",
        "df[df['sku_id'] == 'UYZCL'].iloc[60:, [0, 1, 2, 3, 4, 5, 6, 7]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppj9XHot2UdB"
      },
      "source": [
        "#### These negative values are occurring due to overdue deliveries. We should retain these values to better understand the lead times of our suppliers and improve our demand planning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf729iAP2UdB"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0xclhLS2UdB"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWKgRsGB2UdB"
      },
      "source": [
        "#### Since our goal is to forecast the next 8 weeks, let's keep our dataset until 31-12-2023 to ensure a realistic scenario for the analysis and forecast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gll5Sna2UdB"
      },
      "outputs": [],
      "source": [
        "# Let's have our dataset until 31-12-2023\n",
        "df = df[df['cal_date'] <= '2023-12-31']\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9En96v42UdB"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "print(nbformat.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBPa6iUj2UdB"
      },
      "outputs": [],
      "source": [
        "# How is our demand overtime?\n",
        "fig = px.line(df.groupby('cal_date')['gsales_u'].sum().reset_index(), x='cal_date', y='gsales_u', title='Total Demand Over Time')\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Total Demand', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGRb7AwW2UdB"
      },
      "source": [
        "#### There are remarkable peaks in the sales of some products. Let's check those weeks in specific."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3s1GfCY2UdB"
      },
      "outputs": [],
      "source": [
        "df.groupby('cal_date')['gsales_u'].sum().sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8I96t8y2UdC"
      },
      "outputs": [],
      "source": [
        "# Let's filter these dates to see products and compare on_promo column\n",
        "days = [\"2023-11-26\", \"2022-11-27\", \"2021-11-28\"]\n",
        "\n",
        "others_days = [\"2023-08-15\", \"2023-02-05\", \"2021-08-06\"]\n",
        "\n",
        "# Filter thes edays and check the other columsn\n",
        "print(df[df['cal_date'].isin(days)].iloc[:, 26:][\"on_promo\"].value_counts())\n",
        "print(df[df['cal_date'].isin(others_days)].iloc[:, 26:][\"on_promo\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWzU-nU42UdC"
      },
      "source": [
        "#### After comparing, we can see that our promotions are having a significant impact on sales. These promotions coincide with Black Friday week, which explains the increase in sales.\n",
        "\n",
        "#### What if we analyze sales trends across the months and weeks of the year and evaluate the highest and lowest sales periods?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUUTRrRo2UdC"
      },
      "outputs": [],
      "source": [
        "# Calculate average weekly/monthly gsales_u\n",
        "df['week'] = df['cal_date'].dt.isocalendar().week\n",
        "df['month'] = df['cal_date'].dt.month\n",
        "\n",
        "# Calculate average weekly gsales_u\n",
        "weekly_gsales_u = df.groupby('week')['gsales_u'].mean()\n",
        "\n",
        "# Calculate average monthly gsales_u\n",
        "monthly_gsales_u = df.groupby('month')['gsales_u'].mean()\n",
        "\n",
        "# Plot the average weekly gsales_u\n",
        "fig = px.line(weekly_gsales_u, title='Average Weekly Demand')\n",
        "fig.update_layout(xaxis_title='Week', yaxis_title='Average Demand', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()\n",
        "\n",
        "# Plot the average monthly gsales_u\n",
        "fig = px.line(monthly_gsales_u, title='Average Monthly Demand')\n",
        "fig.update_layout(xaxis_title='Month', yaxis_title='Average Demand', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpKqFV802UdC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0G5s3g12UdC"
      },
      "outputs": [],
      "source": [
        "# Let's plot the sales over months but change the colour for each year, so we can identify any pattern\n",
        "df['year'] = df['cal_date'].dt.year\n",
        "\n",
        "fig = px.line(df.groupby(['year', 'month'])['gsales_u'].sum().reset_index(), x='month', y='gsales_u', color='year', title='Total Demand Over Months')\n",
        "fig.update_layout(xaxis_title='Month', yaxis_title='Total Demand', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()\n",
        "\n",
        "# Let's plot the sales over weeks but change the colour for each year, so we can identify any pattern\n",
        "fig = px.line(df.groupby(['year', 'week'])['gsales_u'].sum().reset_index(), x='week', y='gsales_u', color='year', title='Total Demand Over Weeks')\n",
        "fig.update_layout(xaxis_title='Week', yaxis_title='Total Demand', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx8Mw9Wb2UdC"
      },
      "source": [
        "#### It is difficult to identify seasonality in the weekly data, but when looking at the monthly data, we observe that months 2 and 3 show lower sales, followed by an increase in sales after these months. There is another increase in months 8 and 11 due to promotions. Let's compare the number of promotions across the months."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l35Yvv-I2UdC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgXzxSaS2UdC"
      },
      "outputs": [],
      "source": [
        "# Let's plot the count of promotions over months but change the colour for each year, so we can identify any pattern\n",
        "fig = px.line(df.groupby(['year', 'month'])['on_promo'].sum().reset_index(), x='month', y='on_promo', color='year', title='Total Promotions Over Months')\n",
        "fig.update_layout(xaxis_title='Month', yaxis_title='Total Promotions', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq-j6Egn2UdC"
      },
      "outputs": [],
      "source": [
        "# Let's count on_mkdn\n",
        "fig = px.line(df.groupby(['year', 'month'])['on_mkdn'].sum().reset_index(), x='month', y='on_mkdn', color='year', title='Total Markdowns Over Months')\n",
        "fig.update_layout(xaxis_title='Month', yaxis_title='Total Markdowns', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfS4xa9e2UdC"
      },
      "source": [
        "#### Markdown prices have become more prevalent in recent years. We observed that after August 2021, they started to occur more regularly and are now active year-round. For the on_promo column, we only see this event during the months of November and December. This column will have a significant impact on our forecast and should be included as a predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHxMHvfY2UdD"
      },
      "outputs": [],
      "source": [
        "#### Let's have a look on the items alone and evaluate the time series\n",
        "items = ['QCLLY', 'UYZCL', 'UWYXC', 'QWYXC', 'QWYXC']\n",
        "\n",
        "for item in items:\n",
        "    fig = px.line(df[df['sku_id'] == item], x='cal_date', y='gsales_u', title=f'Trend of Sales for {item}')\n",
        "    fig.update_layout(xaxis_title='Date', yaxis_title='Units Sold', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaKmYIud2UdD"
      },
      "source": [
        "#### Interesting wer have different lenght of time series and some of our products don't have sales. Let's check the products with no sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Yx_8gI2UdD"
      },
      "outputs": [],
      "source": [
        "# Let's find those sku that the sum of gsales_u is 0\n",
        "sum_of_items = df.groupby('sku_id')['gsales_u'].sum().sort_values().reset_index()\n",
        "\n",
        "# Let's filter those with 0 sales\n",
        "zero_sales = sum_of_items[sum_of_items['gsales_u'] == 0]['sku_id'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTh4zs4w2UdD"
      },
      "outputs": [],
      "source": [
        "# How many products in our dataset are with zero sales?\n",
        "#zero_sales = df[(df['gsales_u'] == 0) & (df['dc_reg_inventory_u'] == 0)]\n",
        "\n",
        "# Let's see the percentage of zero sales\n",
        "zero_sales_percentage = (len(zero_sales) / df[\"sku_id\"].nunique()) * 100\n",
        "\n",
        "print(f'The percentage of products with zero sales is {zero_sales_percentage:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNddTqfZ2UdD"
      },
      "source": [
        "### Wow, I wasn't expecting that much. Are this product Out of stock? Let's check the inventory of these products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU-PNLVO2UdD"
      },
      "outputs": [],
      "source": [
        "df[df['sku_id'].isin(zero_sales)].iloc[:, [2, 5]].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OJUChcj2UdD"
      },
      "source": [
        "#### We are going to place these items in a separate dataset and discuss them with the client. We have products with zero inventory that may need to be cleared or delisted, and products with inventory that may require a discussion on whether we need to apply some kind of promotion to sell them.\n",
        "\n",
        "#### This may be due to data entry issues. We will investigate further. For our modeling purposes, we will drop these rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccWJufRW2UdD"
      },
      "outputs": [],
      "source": [
        "# remove the items with 0 sales\n",
        "print(df.shape)\n",
        "df = df[~df['sku_id'].isin(zero_sales)]\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjJibRjN2UdD"
      },
      "outputs": [],
      "source": [
        "#### Let's have a look on the items alone and evaluate the time series\n",
        "items = np.random.choice(df['sku_id'].unique(), 5)\n",
        "\n",
        "for item in items:\n",
        "    fig = px.line(df[df['sku_id'] == item], x='cal_date', y='gsales_u', title=f'Trend of Sales for {item}')\n",
        "    fig.update_layout(xaxis_title='Date', yaxis_title='Units Sold', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGkS3ybU2UdD"
      },
      "outputs": [],
      "source": [
        "df[df['sku_id'] == 'MENEX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYVCA60h2UdD"
      },
      "source": [
        "#### We might have a look on those item with just 1 sale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kf3bqvn2UdD"
      },
      "outputs": [],
      "source": [
        "sum_1_sales = df.groupby('sku_id')['gsales_u'].sum().sort_values().reset_index()\n",
        "\n",
        "# Let's filter those witt less than 5 sales\n",
        "one_sold = sum_1_sales[sum_1_sales['gsales_u'] == 1]['sku_id'].tolist()\n",
        "\n",
        "len(one_sold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVdfUnLk2UdE"
      },
      "source": [
        "#### Our model won't be able to capture any information with these products, we are going to remove them from our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxebBbVU2UdE"
      },
      "outputs": [],
      "source": [
        "# Remove the items with 1 sale\n",
        "print(df.shape)\n",
        "df = df[~df['sku_id'].isin(one_sold)]\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTOQ_knC2UdE"
      },
      "outputs": [],
      "source": [
        "#### Let's have a look on the items alone and evaluate the time series\n",
        "items = np.random.choice(df['sku_id'].unique(), 5)\n",
        "\n",
        "for item in items:\n",
        "    fig = px.line(df[df['sku_id'] == item], x='cal_date', y='gsales_u', title=f'Trend of Sales for {item}')\n",
        "    fig.update_layout(xaxis_title='Date', yaxis_title='Units Sold', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4piOWiQ2UdE"
      },
      "source": [
        "#### Now that we have at least 2 sales for each product. Let's check the correlation between the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY4Y5N8X2UdE"
      },
      "outputs": [],
      "source": [
        "# Let's plot a heatmap for the correlation matrix between our numerical values\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "corr = numeric_df.corr()\n",
        "fig, ax = plt.subplots(figsize=(26, 15))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d3v5MlD2UdE"
      },
      "source": [
        "#### We observe that a few columns have a higher positive correlation with gsales_u, such as gsales_v, inventory, and next_delivery_quantity, which were expected. Even though our sales increased significantly in the last months of the year due to promotions, the algorithm did not capture this information. What else might we be missing? Let's plot the two variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8WBsvd92UdE"
      },
      "outputs": [],
      "source": [
        "# Let's plot gsales_u and on_promo scatter plot and colour the dots with on_mkdn\n",
        "fig = px.scatter(df, x='gsales_u', y='on_promo', color='on_mkdn', title='Scatter Plot of units sold and on_promo with on_mkdn')\n",
        "fig.update_layout(xaxis_title='gsales_u', yaxis_title='on_promo', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBe59DJz2UdE"
      },
      "outputs": [],
      "source": [
        "# How much percentage represents thos sales in promo and markdowns over the total units sold?\n",
        "\n",
        "# Print the percentage of sales in promo and markdowns over the total units sold\n",
        "promo_percentage = df['on_promo'].value_counts(normalize=True) * 100\n",
        "mkdn_percentage = df['on_mkdn'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Percentage of sales in promo:\")\n",
        "print(promo_percentage)\n",
        "print(\"\\nPercentage of sales in markdowns:\")\n",
        "print(mkdn_percentage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNYEEscc2UdE"
      },
      "source": [
        "#### Markdown represent a considerable percentage of the sales. Interesgintly there is no correlation between on_mkdn and gsales_u.\n",
        "\n",
        "#### Let's have a look on the categories and subcategories of the products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_8REtWb2UdE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tqeSpH92UdE"
      },
      "outputs": [],
      "source": [
        "df['division_desc'].value_counts().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8UEs5ze2UdF"
      },
      "outputs": [],
      "source": [
        "# Let's plot our sales by division_desc, department_desc and category_desc\n",
        "fig = px.bar(df['division_desc'].value_counts().reset_index(), x='division_desc', y='count', title='Total Sales by Division')\n",
        "fig.update_layout(xaxis_title='Division', yaxis_title='Total Sales', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(df['department_desc'].value_counts().reset_index(), x='department_desc', y='count', title='Total Sales by Department')\n",
        "fig.update_layout(xaxis_title='Department', yaxis_title='Total Sales', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()\n",
        "\n",
        "fig = px.bar(df['category_desc'].value_counts().reset_index(), x='category_desc', y='count', title='Total Sales by Category')\n",
        "fig.update_layout(xaxis_title='Category', yaxis_title='Total Sales', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2aXRYeY2UdF"
      },
      "outputs": [],
      "source": [
        "# Let's group wich department_desc has the most products in promo and markdowns\n",
        "promo_category = df[df['on_promo'] == 1]['department_desc'].value_counts()\n",
        "mkdn_category = df[df['on_mkdn'] == 1]['department_desc'].value_counts()\n",
        "\n",
        "print(\"Category with most products in promo:\")\n",
        "print(promo_category.head(10))\n",
        "print(\"\\nCategory with most products in markdowns:\")\n",
        "print(mkdn_category.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuv7-3eb2UdF"
      },
      "outputs": [],
      "source": [
        "# Now let's compare the units sold in percentage. How much sales were made with 0 on_promo and 0 on_mkdn? So we can see how much drive has this initiatives\n",
        "# Calculate the percentage of sales with 0 on_promo and 0 on_mkdn\n",
        "total_sales = df['gsales_u'].sum()\n",
        "sales_no_promo_no_mkdn = df[(df['on_mkdn'] == 0)]['gsales_u'].sum()\n",
        "\n",
        "percentage_no_promo_no_mkdn = (sales_no_promo_no_mkdn / total_sales) * 100\n",
        "\n",
        "print(f\"Percentage of sales with 0 on_promo and 0 on_mkdn: {percentage_no_promo_no_mkdn:.2f}%\")\n",
        "# Calculate the percentage of sales with 0 on_promo and 0 on_mkdn for each department_desc\n",
        "department_sales = df.groupby('department_desc')['gsales_u'].sum()\n",
        "department_sales_no_promo_no_mkdn = df[(df['on_mkdn'] == 0)].groupby('department_desc')['gsales_u'].sum()\n",
        "\n",
        "percentage_no_promo_no_mkdn_by_department = (department_sales_no_promo_no_mkdn / department_sales) * 100\n",
        "\n",
        "print(\"Percentage of sales with 0 on_promo and 0 on_mkdn by department:\")\n",
        "print(percentage_no_promo_no_mkdn_by_department)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY7xaCnE2UdF"
      },
      "source": [
        "#### We observe that there is a low percentage of 1 values in the on_mkdn column across departments, so we conclude that promotions and price reductions will not be significant drivers for sales. Therefore, we need to continue exploring other variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kqRhQjb2UdF"
      },
      "outputs": [],
      "source": [
        "# let's examine the inventory levels over time\n",
        "fig = px.line(df.groupby('cal_date')['dc_reg_inventory_u'].sum().reset_index(), x='cal_date', y='dc_reg_inventory_u', title='Total Inventory Over Time')\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Total Inventory', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGGIGyf42UdF"
      },
      "source": [
        "#### This aligned with the sale, where the planner prepare for the seasonality of the last months. Let's have a look on the distribution of the inventory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6i8lQX32UdF"
      },
      "outputs": [],
      "source": [
        "# Let's plot eh distribution of the inventory levels\n",
        "fig = px.histogram(df, x='dc_reg_inventory_u', title='Distribution of Inventory Levels')\n",
        "fig.update_layout(xaxis_title='Inventory Level', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgmEWpod2UdF"
      },
      "outputs": [],
      "source": [
        "# Is there any relation with the gsales_u and the inventory levels?\n",
        "fig = px.scatter(df, x='dc_reg_inventory_u', y='gsales_u', title='Scatter Plot of Inventory Levels and Units Sold')\n",
        "fig.update_layout(xaxis_title='Inventory Level', yaxis_title='Units Sold', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2B_uhis2UdF"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Create a figure with secondary y-axis\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# Add trace for inventory levels\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=df.groupby('cal_date')['dc_reg_inventory_u'].sum().index,\n",
        "               y=df.groupby('cal_date')['dc_reg_inventory_u'].sum(),\n",
        "               name=\"Total Inventory\"),\n",
        "    secondary_y=False,\n",
        ")\n",
        "\n",
        "# Add trace for total sales\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=df.groupby('cal_date')['gsales_u'].sum().index,\n",
        "               y=df.groupby('cal_date')['gsales_u'].sum(),\n",
        "               name=\"Total Sales\"),\n",
        "    secondary_y=True,\n",
        ")\n",
        "\n",
        "# Add figure title and axis labels\n",
        "fig.update_layout(\n",
        "    title_text=\"Total Inventory and Sales Over Time\"\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"Date\")\n",
        "fig.update_yaxes(title_text=\"Total Inventory\", secondary_y=False)\n",
        "fig.update_yaxes(title_text=\"Total Sales\", secondary_y=True)\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcP-E0PW2UdG"
      },
      "source": [
        "#### Interesting how the inbound inventory levels are 3-4 months before the sales. Is that the common lead time? Let's have a look on the concatenate all weeks_to_next_delivery_ columns and see the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dDv4wAk2UdG"
      },
      "outputs": [],
      "source": [
        "# Interesting how the inbound inventory levels are 3'4 months before the sales. Is that the common lead time? Let's have a look on the concatenate all weeks_to_next_delivery_ columns and see the distribution\n",
        "weeks_to_next_delivery = pd.concat([df['weeks_to_next_delivery_1'], df['weeks_to_next_delivery_2'], df['weeks_to_next_delivery_3'], df['weeks_to_next_delivery_4'], df['weeks_to_next_delivery_5']])\n",
        "weeks_to_next_delivery = weeks_to_next_delivery[weeks_to_next_delivery != 0]\n",
        "fig = px.histogram(weeks_to_next_delivery, title='Distribution of Weeks to Next Delivery')\n",
        "fig.update_layout(xaxis_title='Weeks to Next Delivery', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Tz9EY52UdG"
      },
      "source": [
        "# Check later\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKJzs4MG2UdG"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOAoKXkr2UdG"
      },
      "source": [
        "#### Let's have a look on the oo_inventory_u if we can find any patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlE6T6MM2UdG"
      },
      "outputs": [],
      "source": [
        "fig = px.line(df.groupby('cal_date')['oo_inventory_u'].sum().reset_index(), x='cal_date', y='oo_inventory_u', title='Total OO Inventory Over Time')\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Total OO Inventory', template='plotly_white', xaxis_showgrid=False, yaxis_showgrid=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M35TI1P2UdG"
      },
      "source": [
        "#### yep, this is aligned with our inventory levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADfqRuqk2UdG"
      },
      "source": [
        "#### Now we can move to the prices and see how they are distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMzL_vI32UdG"
      },
      "outputs": [],
      "source": [
        "# Let's plot the distribution of the full_price\n",
        "fig = px.histogram(df, x='full_price', title='Distribution of Full Price')\n",
        "fig.update_layout(xaxis_title='Full Price', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()\n",
        "# Plot the distribution of the current_price\n",
        "fig = px.histogram(df, x='current_price', title='Distribution of Current Price')\n",
        "fig.update_layout(xaxis_title='Current Price', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()\n",
        "\n",
        "# Plot the distribution of the ticket_price\n",
        "fig = px.histogram(df, x='ticket_price', title='Distribution of Ticket Price')\n",
        "fig.update_layout(xaxis_title='Ticket Price', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gv4x5iI2UdG"
      },
      "source": [
        "#### As expected we have the lower prices in markdowns. Let's check the distribution of the prices in the products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp1tCT7s2UdG"
      },
      "outputs": [],
      "source": [
        "# Now let's filter those product in on_promo 1 and on_mkdn 1 and see the distribution of the prices\n",
        "fig = px.histogram(df[(df['on_promo'] == 1) & (df['on_mkdn'] == 1)], x='full_price', title='Distribution of Full Price for Products in Promo and Markdowns')\n",
        "fig.update_layout(xaxis_title='Full Price', yaxis_title='Count', template='plotly_white')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MRwJEp32UdG"
      },
      "source": [
        "#### We can confirm that our prices are reduced by the promotions. But how effective are these promotions in gsales_v?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF9a8bGU2UdG"
      },
      "outputs": [],
      "source": [
        "# Calculate the average sales during promotion periods\n",
        "promo_sales = df[df['on_promo'] == 1]['gsales_u'].mean()\n",
        "\n",
        "# Calculate the average sales during non-promotion periods\n",
        "non_promo_sales = df[df['on_promo'] == 0]['gsales_u'].mean()\n",
        "\n",
        "# Calculate the percentage increase in sales during promotion periods\n",
        "percentage_increase = ((promo_sales - non_promo_sales) / non_promo_sales) * 100\n",
        "\n",
        "print(f\"Average sales during promotion periods: {promo_sales:.2f}\")\n",
        "print(f\"Average sales during non-promotion periods: {non_promo_sales:.2f}\")\n",
        "print(f\"Percentage increase in sales during promotion periods: {percentage_increase:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtOe1fqX2UdH"
      },
      "source": [
        "#### After this comparison, we can confirm and recommend advising the business to inform us whenever they plan to run a promotion, so we can improve the forecast for those periods. The percentage increase in sales is a key factor for our analysis and provides valuable insights for the forecast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3uPr1J2UdH"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN13yO0L2UdH"
      },
      "source": [
        "#### We already have month and week. Let's add if the week is the last week of the month. This could be a driver for the sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GYV3rYQ2UdH"
      },
      "outputs": [],
      "source": [
        "# Let's add if the week is the last of the month or not\n",
        "df['last_week_of_month'] = df['cal_date'].dt.is_month_end\n",
        "\n",
        "# Because we have the size code, maybe we are deailing with clothes. Let's add the season column\n",
        "df['season'] = df['cal_date'].dt.quarter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTFarfAz2UdH"
      },
      "source": [
        "#### Price feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75T3D9GC2UdH"
      },
      "outputs": [],
      "source": [
        "# Let's add the discount and the discount percentage. This will give us an idea if the percentage of discount is related to the sales\n",
        "df['price_discount'] = df['full_price'] - df['current_price']\n",
        "df['discount_percentage'] = (df['price_discount'] / df['full_price'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcp12wiW2UdH"
      },
      "outputs": [],
      "source": [
        "# Now let's add the promotion days is going to be that product\n",
        "df['total_discount_days'] = df['promo_days_this_week'] + df['mkdn_days_this_week']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIhcDlXJ2UdH"
      },
      "source": [
        "### Inventory feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTh2nfiJ2UdH"
      },
      "outputs": [],
      "source": [
        "# Inventory ratios\n",
        "df['total_incoming_inventory'] = (df['next_delivery_quantity_1'] +\n",
        "                                df['next_delivery_quantity_2'] +\n",
        "                                df['next_delivery_quantity_3'] +\n",
        "                                df['next_delivery_quantity_4'] +\n",
        "                                df['next_delivery_quantity_5'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjeGkEz2UdH"
      },
      "source": [
        "#### Categorical feture engineering\n",
        "\n",
        "##### On this one we have 4, division_desc, department_desc, category_desc and size_code. We are going to use the get_dummies to transform these columns into numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7TtmZxj2UdH"
      },
      "outputs": [],
      "source": [
        "# let's encode our categorical columns\n",
        "df = pd.get_dummies(df, columns=['division_desc', 'department_desc', 'category_desc', 'size_code'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUzm-ctg2UdI"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhhL9h9u2UdI"
      },
      "outputs": [],
      "source": [
        "# Now let's evaluate again the correlation matrix\n",
        "\n",
        "numeric_df_2 = df.select_dtypes(include=[np.number])\n",
        "corr = numeric_df_2.iloc[:, :44].corr()\n",
        "fig, ax = plt.subplots(figsize=(26, 15))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O1FYQOi2UdI"
      },
      "source": [
        "#### Because our linear correlation doesn't provide additional insights, let's create a simple decision tree model to see if we can capture any non-linear relationships between the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n4kpJJv2UdI"
      },
      "outputs": [],
      "source": [
        "# Define the features (X) and the target variable (y)\n",
        "# Because sales are usually corrlated with the numbers of units sold, I am going to exclude from the basic regression model to evaluate the new features\n",
        "X = df.drop(columns=['gsales_u', 'gsales_v', 'sku_id', 'cal_date', 'next_delivery_date_1', 'next_delivery_date_2', 'next_delivery_date_3', 'next_delivery_date_4', 'next_delivery_date_5'])\n",
        "y = df['gsales_u']\n",
        "\n",
        "# Feature importance (using a simple model)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "rf.fit(X, y)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvkew0Ls2UdI"
      },
      "source": [
        "#### That't interesting that our inventory is one of the important for our independent variables. Let's filter just those one grater than 0.015"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxqVz9Qf2UdI"
      },
      "outputs": [],
      "source": [
        "# Let's print out the filter\n",
        "print(feature_importance[feature_importance['importance'] > 0.015])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMaZfQW12UdI"
      },
      "source": [
        "#### This aligns with the previous plot we created between sales and inventory. Our lead times and inventory increases serve as indicators that we are entering the season when our sales increase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X9i-3VY2UdI"
      },
      "outputs": [],
      "source": [
        "# Let's save in a list the features importance that are higher than 0.015\n",
        "selected_features = feature_importance[feature_importance['importance'] > 0.015]['feature'].tolist()\n",
        "\n",
        "# Now let's filter our dataset with different name, needs to include as well the target variable, dates and sku_id\n",
        "df_filtered = df[['gsales_u', 'sku_id', 'cal_date'] + selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJKUWq0S2UdI"
      },
      "outputs": [],
      "source": [
        "df_filtered.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7eC3b1j2UdI"
      },
      "source": [
        "#### We can see that week and category has different dtypes, let's change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iiMqpLs2UdI"
      },
      "outputs": [],
      "source": [
        "# Week and month should be integer64\n",
        "df_filtered.loc[:, 'week'] = df_filtered['week'].astype('int64')\n",
        "df_filtered.loc[:, 'month'] = df_filtered['month'].astype('int64')\n",
        "df_filtered.loc[:, 'year'] = df_filtered['year'].astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqushi0U2UdJ"
      },
      "outputs": [],
      "source": [
        "# Let's plot the scatter plot of each feature with the target variable\n",
        "for feature in selected_features:\n",
        "    fig = px.scatter(df_filtered, x=feature, y='gsales_u', title=f'Scatter Plot of {feature} and Units Sold')\n",
        "    fig.update_layout(xaxis_title=feature, yaxis_title='Units Sold', template='plotly_white')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3YIsBRu2UdJ"
      },
      "source": [
        "#### After evaluating our features, none of them show statistical significance for our model. We are now going to try incorporating lag features from our target variable and assess their importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p3GXYLC2UdJ"
      },
      "source": [
        "#### For this case, that we have many time series we are going to model univartie time series and multivariate time series.\n",
        "\n",
        "#### But first let's create our benchmark model. We are going to use the mean of the sales of the last 8 weeks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DqgkHSH2UdJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure the 'cal_date' column is in datetime format\n",
        "df_filtered['cal_date'] = pd.to_datetime(df_filtered['cal_date'])\n",
        "\n",
        "# Create the moving average for the past 8 weeks, shifted to exclude the current week\n",
        "df_filtered['mean_8_weeks'] = df_filtered.groupby('sku_id')['gsales_u'].transform(\n",
        "    lambda x: x.shift(1).rolling(8, min_periods=1).mean()\n",
        ")\n",
        "\n",
        "# Split into train and test datasets\n",
        "train = df_filtered[df_filtered['cal_date'] <= '2023-11-12']\n",
        "test = df_filtered[df_filtered['cal_date'] > '2023-11-12']\n",
        "\n",
        "# Extract the last row of training data for each SKU (latest known mean_8_weeks value)\n",
        "train_last_means = train.groupby('sku_id')[['mean_8_weeks']].last().reset_index()\n",
        "\n",
        "# Prepare the test set for the next 8 weeks\n",
        "test_8_weeks = pd.DataFrame()\n",
        "start_date = pd.to_datetime('2023-11-12')\n",
        "for i in range(1, 9):\n",
        "    # Generate the date for each week\n",
        "    target_date = start_date + pd.DateOffset(weeks=i)\n",
        "    week_data = test[test['cal_date'] == target_date]\n",
        "\n",
        "    if not week_data.empty:\n",
        "        test_8_weeks = pd.concat([test_8_weeks, week_data], axis=0)\n",
        "\n",
        "# Merge the last means from train into the test_8_weeks dataset\n",
        "test_8_weeks = test_8_weeks.merge(train_last_means, on='sku_id', how='left')\n",
        "\n",
        "# Compare the benchmark predictions with actual sales\n",
        "test_8_weeks['benchmark_error'] = (test_8_weeks['gsales_u'] - test_8_weeks['mean_8_weeks_y']).abs()\n",
        "\n",
        "# Evaluate the benchmark model (e.g., using Mean Absolute Error)\n",
        "mae_benchmark = test_8_weeks['benchmark_error'].mean()\n",
        "\n",
        "print(f\"Mean Absolute Error of the benchmark model: {mae_benchmark}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bifz9ehA2UdJ"
      },
      "outputs": [],
      "source": [
        "sku = np.random.choice(test_8_weeks['sku_id'].unique(), 5)\n",
        "\n",
        "for item in sku:\n",
        "    fig = make_subplots(rows=1, cols=1)\n",
        "    fig.add_trace(go.Scatter(x=test_8_weeks[test_8_weeks['sku_id'] == item]['cal_date'], y=test_8_weeks[test_8_weeks['sku_id'] == item]['gsales_u'], name='Actual', mode='lines', marker=dict(color='blue')), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=test_8_weeks[test_8_weeks['sku_id'] == item]['cal_date'], y=test_8_weeks[test_8_weeks['sku_id'] == item]['mean_8_weeks_y'], name='Predicted', mode='lines', marker=dict(color='red')), row=1, col=1)\n",
        "    fig.update_layout(title=f'Actual vs Predicted for {item}', xaxis_title='Date', yaxis_title='Units Sold', template='plotly_white')\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvwDwtrJ2UdJ"
      },
      "source": [
        "#### Now that we have our benchmark, let's go with the univariate time series and multivariate time series to evaluate if we can beat the benchmark. 16.58%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2eVPdpD2UdJ"
      },
      "source": [
        "#### We are going to use the library Skforecast who can help us to achieve this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhLzJBDb2UdJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "color = '\\033[1m\\033[38;5;208m'\n",
        "print(f\"{color}Version skforecast: {skforecast.__version__}\")\n",
        "print(f\"{color}Version scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"{color}Version pandas: {pd.__version__}\")\n",
        "print(f\"{color}Version numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLl-OYLL2UdJ"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "# ======================================================================================\n",
        "selected_items = df_filtered.sku_id.unique()\n",
        "data = df_filtered[(df_filtered['sku_id'].isin(selected_items))].copy()\n",
        "data['date'] = pd.to_datetime(data['cal_date'], format='%Y-%m-%d')\n",
        "data = pd.pivot_table(\n",
        "           data    = data,\n",
        "           values  = 'gsales_u',\n",
        "           index   = 'date',\n",
        "           columns = 'sku_id'\n",
        "       )\n",
        "data.columns.name = None\n",
        "data.columns = [f\"{col}\" for col in data.columns]\n",
        "data = data.asfreq('1W')\n",
        "data = data.sort_index()\n",
        "# replace NaN values with 0\n",
        "data = data.fillna(0)\n",
        "data.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uek78Yu2UdJ"
      },
      "outputs": [],
      "source": [
        "# Split data into train-validation-test\n",
        "# ======================================================================================\n",
        "end_train = '2023-09-17'\n",
        "end_val = '2023-11-12'\n",
        "data_train = data.loc[:end_train, :].copy()\n",
        "data_val   = data.loc[end_train:end_val, :].copy()\n",
        "data_test  = data.loc[end_val:, :].copy()\n",
        "print(f\"Train dates      : {data_train.index.min()} --- {data_train.index.max()}  (n={len(data_train)})\")\n",
        "print(f\"Validation dates : {data_val.index.min()} --- {data_val.index.max()}  (n={len(data_val)})\")\n",
        "print(f\"Test dates       : {data_test.index.min()} --- {data_test.index.max()}  (n={len(data_test)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XctPwVVL2UdJ"
      },
      "outputs": [],
      "source": [
        "# Plot time series\n",
        "# ======================================================================================\n",
        "set_dark_theme()\n",
        "fig, axs = plt.subplots(4, 1, figsize=(7, 5), sharex=True)\n",
        "data.iloc[:, :4].plot(\n",
        "    legend   = True,\n",
        "    subplots = True,\n",
        "    title    = 'Unis sales',\n",
        "    ax       = axs,\n",
        ")\n",
        "for ax in axs:\n",
        "    ax.axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1.5)\n",
        "    ax.axvline(pd.to_datetime(end_val) , color='white', linestyle='--', linewidth=1.5)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIhaG2_O2UdK"
      },
      "outputs": [],
      "source": [
        "# Train and backtest a model for each item: ForecasterAutoreg\n",
        "items = []\n",
        "mae_values = []\n",
        "predictions = {}\n",
        "\n",
        "for i, item in enumerate(tqdm(data.columns)):\n",
        "    # Define forecaster\n",
        "    window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=4)\n",
        "    forecaster = ForecasterRecursive(\n",
        "                     regressor       = HistGradientBoostingRegressor(random_state=42),\n",
        "                     lags            = 2,\n",
        "                     window_features = window_features\n",
        "                 )\n",
        "    # Backtesting forecaster\n",
        "    cv = TimeSeriesFold(\n",
        "            steps              = 1,\n",
        "            initial_train_size = len(data_train) + len(data_val),\n",
        "            refit              = False,\n",
        "         )\n",
        "    metric, preds = backtesting_forecaster(\n",
        "                        forecaster    = forecaster,\n",
        "                        y             = data[item],\n",
        "                        cv            = cv,\n",
        "                        metric        = 'mean_absolute_error',\n",
        "                        verbose       = False,\n",
        "                        show_progress = False\n",
        "                    )\n",
        "    items.append(item)\n",
        "    mae_values.append(metric.at[0, 'mean_absolute_error'])\n",
        "    predictions[item] = preds\n",
        "\n",
        "# Results\n",
        "uni_series_mae = pd.Series(\n",
        "                     data  = mae_values,\n",
        "                     index = items,\n",
        "                     name  = 'uni_series_mae'\n",
        "                 )\n",
        "uni_series_mae.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDMW4gjF2UdK"
      },
      "outputs": [],
      "source": [
        "# Train and backtest a model for all items: ForecasterAutoregMultiSeries\n",
        "items = list(data.columns)\n",
        "\n",
        "# Define forecaster\n",
        "window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=4)\n",
        "forecaster_ms = ForecasterRecursiveMultiSeries(\n",
        "                    regressor          = HistGradientBoostingRegressor(random_state=42),\n",
        "                    lags               = 2,\n",
        "                    encoding           = 'ordinal',\n",
        "                    transformer_series = StandardScaler(),\n",
        "                    window_features    = window_features,\n",
        "                )\n",
        "# Backtesting forecaster for all items\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 1,\n",
        "        initial_train_size = len(data_train) + len(data_val),\n",
        "        refit              = False,\n",
        "     )\n",
        "multi_series_mae, predictions_ms = backtesting_forecaster_multiseries(\n",
        "                                       forecaster            = forecaster_ms,\n",
        "                                       series                = data,\n",
        "                                       levels                = items,\n",
        "                                       cv                    = cv,\n",
        "                                       metric                = 'mean_absolute_error',\n",
        "                                       add_aggregated_metric = False,\n",
        "                                       verbose               = False,\n",
        "                                       show_progress         = True\n",
        "                                   )\n",
        "# Results\n",
        "display(multi_series_mae.head(3))\n",
        "print('')\n",
        "display(predictions_ms.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8N1cBFQ2UdK"
      },
      "outputs": [],
      "source": [
        "# Difference of backtesting metric for each item\n",
        "multi_series_mae = multi_series_mae.set_index('levels')\n",
        "multi_series_mae.columns = ['multi_series_mae']\n",
        "results = pd.concat((uni_series_mae, multi_series_mae), axis = 1)\n",
        "results['improvement'] = results.eval('uni_series_mae - multi_series_mae')\n",
        "results['improvement_(%)'] = 100 * results.eval('(uni_series_mae - multi_series_mae) / uni_series_mae')\n",
        "results = results.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agyomGSR2UdK"
      },
      "outputs": [],
      "source": [
        "# Let's plot the gsales_u from the items: ZZUMC, ZYWEM, ZYLQC\n",
        "items = ['AAXDT', 'ZYWEM', 'ZYLQC']\n",
        "fig, axs = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\n",
        "data.loc[:, items].plot(\n",
        "    legend   = True,\n",
        "    subplots = True,\n",
        "    title    = 'Unis sales',\n",
        "    ax       = axs,\n",
        ")\n",
        "for ax in axs:\n",
        "    ax.axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1.5)\n",
        "    ax.axvline(pd.to_datetime(end_val) , color='white', linestyle='--', linewidth=1.5)\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "# Plot predictions from the time series alone and the global model\n",
        "fig, axs = plt.subplots(3, 1, figsize=(15, 8), sharex=True)\n",
        "\n",
        "# Plot actual sales\n",
        "data.loc[:, items].plot(\n",
        "    legend   = True,\n",
        "    subplots = True,\n",
        "    title    = 'Units sales and Predictions',\n",
        "    ax       = axs,\n",
        ")\n",
        "\n",
        "# Plot predictions from the time series alone\n",
        "for i, item in enumerate(items):\n",
        "    predictions[item].plot(ax=axs[i], linestyle='--', color='orange', label='Time Series Alone Prediction')\n",
        "\n",
        "# Plot predictions from the global model\n",
        "for i, item in enumerate(items):\n",
        "    predictions_ms[item].plot(ax=axs[i], linestyle='--', color='green', label='Global Model Prediction')\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axvline(pd.to_datetime(end_train) , color='white', linestyle='--', linewidth=1.5)\n",
        "    ax.axvline(pd.to_datetime(end_val) , color='white', linestyle='--', linewidth=1.5)\n",
        "    ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd6TADTO2UdK"
      },
      "outputs": [],
      "source": [
        "# Average improvement for all items\n",
        "# ======================================================================================\n",
        "# Let's replace inf in results 'improvement_(%)' with 0 because it means that the model is worse than the time series alone\n",
        "results = results.replace([np.inf, -np.inf], 0)\n",
        "# Let's calculate the average mae of both models\n",
        "average_mae = results.mean()\n",
        "average_mae = average_mae.round(2)\n",
        "average_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_LhZx3w2UdK"
      },
      "source": [
        "#### By averaging the results of both models, we can see that our performance is better when using a global model. It's also worth mentioning the computational power required to generate this forecast and for the production process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adu9uIcp2UdK"
      },
      "source": [
        "#### What if we hyper-tune the model? With the help of the library, we can use cv_search to find the best parameters for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6XOo8352UdK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter search for the multi-series model and backtesting for each item\n",
        "def search_space(trial):\n",
        "    search_space  = {\n",
        "        'lags'          : trial.suggest_categorical('lags', [2, 8]),\n",
        "        'max_iter'      : trial.suggest_int('max_iter', 100, 500),\n",
        "        'max_depth'     : trial.suggest_int('max_depth', 2, 7),\n",
        "        'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.1)\n",
        "    }\n",
        "\n",
        "    return search_space\n",
        "\n",
        "cv_search = OneStepAheadFold(initial_train_size = len(data_train))\n",
        "\n",
        "cv_backtesting = TimeSeriesFold(\n",
        "                        steps              = 2,\n",
        "                        initial_train_size = len(data_train) + len(data_val),\n",
        "                        refit              = False,\n",
        "                        )\n",
        "\n",
        "window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=4)\n",
        "forecaster_ms = ForecasterRecursiveMultiSeries(\n",
        "                    regressor          = HistGradientBoostingRegressor(random_state=42),\n",
        "                    lags               = 2,\n",
        "                    window_features    = window_features,\n",
        "                    transformer_series = StandardScaler(),\n",
        "                    encoding           = 'ordinal'\n",
        "                )\n",
        "\n",
        "warnings.simplefilter('ignore', category=OneStepAheadValidationWarning)\n",
        "results_bayesian_ms = bayesian_search_forecaster_multiseries(\n",
        "                        forecaster    = forecaster_ms,\n",
        "                        series        = data.loc[:end_val, :],\n",
        "                        levels        = None, # If is it None select all levels\n",
        "                        cv            = cv_search,\n",
        "                        search_space  = search_space,\n",
        "                        n_trials      = 10,\n",
        "                        metric        = 'mean_absolute_error',\n",
        "                        return_best   = True,\n",
        "                        verbose       = False,\n",
        "                        show_progress = False\n",
        "                    )\n",
        "\n",
        "multi_series_mae, predictions_ms = backtesting_forecaster_multiseries(\n",
        "                                       forecaster            = forecaster_ms,\n",
        "                                       series                = data,\n",
        "                                       levels                = None, # If is it None select all levels\n",
        "                                       cv                    = cv_backtesting,\n",
        "                                       metric                = 'mean_absolute_error',\n",
        "                                       add_aggregated_metric = False,\n",
        "                                       verbose               = False\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQiDOLYx2UdK"
      },
      "outputs": [],
      "source": [
        "# Train and backtest a model for all items: ForecasterAutoregMultiSeries\n",
        "items = list(data.columns)\n",
        "\n",
        "# Define forecaster\n",
        "window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=4)\n",
        "forecaster_ms = ForecasterRecursiveMultiSeries(\n",
        "                    regressor          = HistGradientBoostingRegressor(\n",
        "                                            max_iter=275,\n",
        "                                            max_depth=2,\n",
        "                                            learning_rate=0.04582398297973883,\n",
        "                                            random_state=42),\n",
        "                    lags               = [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "                    encoding           = 'ordinal',\n",
        "                    transformer_series = StandardScaler(),\n",
        "                    window_features    = window_features,\n",
        "                )\n",
        "\n",
        "# Backtesting forecaster for all items\n",
        "cv = TimeSeriesFold(\n",
        "        steps              = 1,\n",
        "        initial_train_size = len(data_train) + len(data_val),\n",
        "        refit              = False,\n",
        "     )\n",
        "multi_series_mae, predictions_ms = backtesting_forecaster_multiseries(\n",
        "                                       forecaster            = forecaster_ms,\n",
        "                                       series                = data,\n",
        "                                       levels                = items,\n",
        "                                       cv                    = cv,\n",
        "                                       metric                = 'mean_absolute_error',\n",
        "                                       add_aggregated_metric = False,\n",
        "                                       verbose               = False,\n",
        "                                       show_progress         = True\n",
        "                                   )\n",
        "# Results\n",
        "display(multi_series_mae.head(3))\n",
        "print('')\n",
        "display(predictions_ms.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXwdptwZ2UdL"
      },
      "outputs": [],
      "source": [
        "# Difference of backtesting metric for each item\n",
        "multi_series_mae = multi_series_mae.set_index('levels')\n",
        "multi_series_mae.columns = ['multi_series_mae']\n",
        "results_cv = pd.concat((uni_series_mae, multi_series_mae), axis = 1)\n",
        "results_cv['improvement'] = results.eval('uni_series_mae - multi_series_mae')\n",
        "results_cv['improvement_(%)'] = 100 * results.eval('(uni_series_mae - multi_series_mae) / uni_series_mae')\n",
        "results_cv = results_cv.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNCOnf702UdL"
      },
      "outputs": [],
      "source": [
        "# Average improvement for all items\n",
        "\n",
        "# Let's replace inf in results 'improvement_(%)' with 0 because it means that the model is worse than the time series alone\n",
        "results_cv = results_cv.replace([np.inf, -np.inf], 0)\n",
        "# Let's calculate the average mae of both models\n",
        "average_mae_cv = results_cv.mean()\n",
        "average_mae_cv = average_mae_cv.round(2)\n",
        "average_mae_cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mia_b1Nr2UdL"
      },
      "source": [
        "#### After hyper-tuning the multi-series model, we can see that its performance is better than the previous one. We can now use this model to forecast the sales of the products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SlrcaTo2UdL"
      },
      "outputs": [],
      "source": [
        "# Now that we have our best model, let's predict the next 8 weeks\n",
        "\n",
        "# Define forecaster\n",
        "window_features = RollingFeatures(stats=['mean', 'min', 'max'], window_sizes=4)\n",
        "\n",
        "forecaster_ms = ForecasterRecursiveMultiSeries(\n",
        "                    regressor          = HistGradientBoostingRegressor(\n",
        "                                            max_iter=275,\n",
        "                                            max_depth=2,\n",
        "                                            learning_rate=0.04582398297973883,\n",
        "                                            random_state=42),\n",
        "                    lags               = [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "                    encoding           = 'ordinal',\n",
        "                    transformer_series = StandardScaler(),\n",
        "                    window_features    = window_features,\n",
        "                )\n",
        "\n",
        "# Fit forecaster\n",
        "forecaster_ms.fit(data, exog=None)\n",
        "\n",
        "# Predict next 8 weeks\n",
        "steps = 8\n",
        "\n",
        "predictions = forecaster_ms.predict(steps = steps, last_window = data.iloc[-8:, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT9rCGC82UdL"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p--Wlvme2UdL"
      },
      "outputs": [],
      "source": [
        "# Let's upload our last 8 week from the csv file\n",
        "\n",
        "# Load the data\n",
        "data_8_weeks = pd.read_csv('MasterDataTEST_OP.csv', low_memory=False)\n",
        "\n",
        "# Convert the date column to datetime\n",
        "data_8_weeks['cal_date'] = pd.to_datetime(data_8_weeks['cal_date'])\n",
        "\n",
        "# Filter the data for the last 8 weeks\n",
        "data_8_weeks = data_8_weeks[data_8_weeks['cal_date'] > '2024-01-01']\n",
        "\n",
        "# Let's filter the items that we have in our predictions and in the data\n",
        "items = predictions.columns\n",
        "\n",
        "data_8_weeks = data_8_weeks[data_8_weeks['sku_id'].isin(items)]\n",
        "\n",
        "# Pivot the data\n",
        "data_8_weeks = pd.pivot_table(\n",
        "                 data    = data_8_weeks,\n",
        "                 values  = 'gsales_u',\n",
        "                 index   = 'cal_date',\n",
        "                 columns = 'sku_id'\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkysKj5l2UdL"
      },
      "outputs": [],
      "source": [
        "# let's calculate the MAE with the actual sales from data_8_weeks and the predictions\n",
        "mae = np.abs(data_8_weeks - predictions).mean().mean()\n",
        "\n",
        "print(f\"Mean Absolute Error for the next 8 weeks: {mae:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ee0xaz2UdL"
      },
      "source": [
        "#### Great our model performed better than our benchmark and without including the exogenous variables.\n",
        "\n",
        "\n",
        "#### Let's plot some of our items to compare the sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq-acAi22UdL"
      },
      "outputs": [],
      "source": [
        "# Let's plot some of the predictions\n",
        "items = np.random.choice(data_8_weeks.columns, 5)\n",
        "\n",
        "fig, axs = plt.subplots(5, 1, figsize=(15, 8), sharex=True)\n",
        "\n",
        "for i, item in enumerate(items):\n",
        "    data_8_weeks[item].plot(ax=axs[i], color='green', label='Actual')\n",
        "    predictions[item].plot(ax=axs[i], linestyle='--', color='orange', label='Predicted')\n",
        "    axs[i].set_title(item)\n",
        "    axs[i].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKBxfe7M2UdL"
      },
      "source": [
        "#### As we can observe in the chart, our model is generally able to capture the sales trend. There is significant room for improvement in the forecast, and more feature engineering can be tested, but the results from this multi-series model provide a good starting point for our client.\n",
        "\n",
        "#### What I always like to tell them is: they can focus on analyzing the products and the forecast output to identify those that weren't captured well by the model and make adjustments, as they have more knowledge of the operations and suppliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXcWZ_QF2UdL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "skforecast_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}